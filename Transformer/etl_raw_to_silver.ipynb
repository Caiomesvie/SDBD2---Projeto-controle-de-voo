{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8062844",
   "metadata": {},
   "source": [
    "# ETL da camada bronze para camada silver\n",
    "\n",
    "Este notebook realiza o ETL dos dados da camada bronze para a camada silver. Ou seja: ele abre o dataset e o salva num dataframe, realiza a transformação dos dados e os carrega num arquivo `.csv` e no banco de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77057",
   "metadata": {},
   "source": [
    "## EXTRACT\n",
    "# Estratégia de Leitura Escalável (PySpark)\n",
    "\n",
    "Para contornar as limitações de memória (especialmente em ambientes como WSL) ao ler o arquivo `MICRODADOS_ENEM_2021.csv` (>1.5GB), adotamos a estratégia de processamento distribuído com **Apache Spark**.\n",
    "\n",
    "Diferente do padrão do Pandas que tenta carregar todo o arquivo na RAM de uma vez, o PySpark utiliza o conceito de *Lazy Evaluation* (avaliação preguiçosa). Ele mapeia o arquivo e cria um plano de execução, mas só processa os dados na memória quando uma ação é solicitada, evitando o travamento do sistema.\n",
    "\n",
    "#### **Parâmetros Críticos:**\n",
    "\n",
    "* **`inferSchema=\"true\"`:** Permite que o Spark percorra os dados inicialmente para identificar automaticamente quais colunas são numéricas e quais são textuais, facilitando a análise imediata.\n",
    "* **`encoding=\"ISO-8859-1\"`:** Corrige erros de decodificação de caracteres que contem dentro do arquivo `MICRODADOS_ENEM_2021.csv` que e um \"erro\" bem comuns em dados  usado no Brasil.\n",
    "* **`delimiter=\";\"`:** Define o ponto e vírgula como o separador correto, evitando que o dataset seja interpretado equivocadamente como uma única coluna longa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326801bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RawToSilves\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_layer_filepath = '../raw/'\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_layer_filepath + 'data_raw/MICRODADOS_ENEM_2021.csv')\n",
    "\n",
    "print(\"Arquivo completo mapeado com sucesso!\")\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "##df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421627a",
   "metadata": {},
   "source": [
    "## TRANSFORM\n",
    "### Padronização dos Nomes das Colunas.\n",
    "\n",
    "Inicialmente, o dataset contém colunas com nomes sem padrão, por \"sorte\" nessa basse que estamos usando todos os nome estão padronizado porem tudo com letras Maisculas. E para fins de tratamento iremos fazer uma varedura para que caso ajá alguma coluna que não eteja separam palavras com `_`, ou tenha alguma coluna com `whitespace`, passse para o padrão com as colunas seguindo esse novo padrão: **todos os caracteres em minúsculo, separando palavras com `_`**.\n",
    "\n",
    "\n",
    "Para renomear todas as colunas de uma vez convertendo para minúsculas e trocando espaços por `_`, a maneira mais eficiente quando se usa o **PySpark** é usar o método `.toDF().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15443db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "novas_colunas = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "df = df.toDF(*novas_colunas)\n",
    "\n",
    "print(df.columns)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755801b",
   "metadata": {},
   "source": [
    "### Remoção de Colunas Desnecessárias\n",
    "\n",
    "Afim de melhorar como os dados vão ficar nessa etapa começamos a filtrar e retirar alguns dados são desnecesarios como por exemplo **nu_ano** já que toda a base e referente ao ano de 2021, e assim optamos por não trabalhar com essa coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5aa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['nu_ano']\n",
    "\n",
    "df = df.drop(*cols_to_drop) \n",
    "\n",
    "for col in cols_to_drop:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Coluna {col} deletada!\")\n",
    "\n",
    "print(\"Colunas restantes: \")\n",
    "print(df.columns)\n",
    "print(\" \")\n",
    "print(\"Tabelas restantes: \")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from psycopg import connect, sql\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "\n",
    "DB_SCHEMA = \"silver\" \n",
    "TABLE_NAME = \"dados_inep\" \n",
    "TABLE_FULL_NAME = f\"{DB_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "print(\"--- Iniciando processo de carga e verificação no PostgreSQL ---\")\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    ddl_path = data_layer_filepath + 'silver/silver_ddl.sql' \n",
    "    ddl = open(ddl_path).read().replace('\\n', ' ')\n",
    "except Exception as e:\n",
    "    print(f'Erro ao abrir arquivo ddl: {e}')\n",
    "    ddl = None \n",
    "\n",
    "def get_db_connection_info():\n",
    "    load_dotenv()\n",
    "    \n",
    "    url = os.getenv('DB_URL')\n",
    "    db_env = os.getenv('DB_ENV')\n",
    "    if url is not None and db_env == 'prod':\n",
    "        return url\n",
    "\n",
    "   \n",
    "    DB_USER = \"admin\"           \n",
    "    DB_PASSWORD = \"l1l2r1r2\"    \n",
    "    DB_HOST = \"localhost\"       \n",
    "    DB_PORT = \"5432\"\n",
    "    DB_NAME = \"dados_inep\"\n",
    "\n",
    "    conn_string = f\"host={DB_HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\"\n",
    "    return conn_string\n",
    "\n",
    "conn_info = get_db_connection_info()\n",
    "\n",
    "\n",
    "print(f\"Total de linhas a serem carregadas: {len(df)}\")\n",
    "\n",
    "cols = list(df.columns)\n",
    "\n",
    "insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "    sql.Identifier(DB_SCHEMA, TABLE_NAME),                      \n",
    "    sql.SQL(\", \").join(map(sql.Identifier, cols)),               \n",
    "    sql.SQL(\", \").join(sql.Placeholder() * len(cols))            \n",
    ")\n",
    "\n",
    "def get_number_of_rows(cur):\n",
    "    \n",
    "    try:\n",
    "        query = sql.SQL('select count(*) from {};').format(sql.Identifier(DB_SCHEMA, TABLE_NAME))\n",
    "        cur.execute(query)\n",
    "        return cur.fetchone()[0]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "with connect(conn_info) as conn:\n",
    "    print(\"\\nConexão com o PostgreSQL (dados_inep) estabelecida.\")\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql.SQL(\"CREATE SCHEMA IF NOT EXISTS {};\").format(sql.Identifier(DB_SCHEMA)))\n",
    "        \n",
    "        if ddl:\n",
    "            print(\"Aplicando DDL...\")\n",
    "            cur.execute(ddl)\n",
    "            conn.commit()\n",
    "            print(\"Estrutura do banco de dados verificada.\")\n",
    "        \n",
    "        print(f'Existem {get_number_of_rows(cur)} linhas na tabela {TABLE_FULL_NAME}!')\n",
    "        print(\"Iniciando carga de dados...\")\n",
    "\n",
    "        \n",
    "        counter = 0\n",
    "        for _, row in df.iterrows():\n",
    "            values = [None if pd.isna(v) else v for v in row]\n",
    "            cur.execute(insert_query, values)\n",
    "            \n",
    "            counter += 1\n",
    "            if counter % 10000 == 0:\n",
    "                conn.commit()\n",
    "                print(f\"{counter} linhas processadas...\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Carga concluída com sucesso!\")\n",
    "        print(f'Total final: {get_number_of_rows(cur)} linhas no banco de dados!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
